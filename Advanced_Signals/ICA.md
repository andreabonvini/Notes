### Independent Component Analysis

ICA. Negentropy. Projection pursuit, qual è la novità? Che trovo le proiezioni in modo iterativo, una dopo l’altra, e non tutte insieme. Ne trovo una, la sottraggo per trovare la seconda, e cosi via. 

- *What is Blind Source Separation?*

  Blind Source Separation is the separation of a set of source signals from a set of mixed signals, without the aid of information (or with very little information) about the source signals or the mixing process. 

*Independent Component Analysis* (ICA) attempts to decompose a multivariate signal into *independent non-gaussian signals*.

This technique gives very good results under these $2$ assumptions:

- The source signals are independent of each other
- The values in each source signal have non-gaussian distributions

Moreover, this technique is based on $3​$ effects of mixing source signals:

- *Independence*: even if the source signals are independent, their signal mixtures are not because they share the same source signals. 
- *Normality*: According to the Central Limit Theorem, the distribution of a sum of independent random variables with finite variance tends towards a gaussian distribution. Loosely speaking, a sum of two independent random variables (signals) usually has a distribution that is closer to gaussian than any of the two original variables. 
- *Complexity*: The temporal complexity of any signal mixture is greater that that of its simplest constituent source signal

From this last effect we expect that our *sources* are likely characterized by a low complexity.

But how does *ICA* find these sources?

*ICA* finds the independent components (also called factors, latent variables or sources) by maximizing the statistical independence of the estimated components.

The two broadest definitions of independence for *ICA* are

- **Minimization of Mutual Information** (MMI)

  This family use measures like the *Kullback-Leibler Divergence* and *Maximum Entropy*

- **Maximization of non-Gaussianity**

  This family use measures like the *Kurtosis* and the *Negentropy*

Typical algorithms for *ICA* use *centering*  (subtract the mean to create a zero mean signal), *whitening* (usually with the eigenvalue decomposition), and *dimensionality reduction* as preprocessing steps in order to simplify and reduce the complexity of the problem for the actual iterative algorithm. Whitening and dimensionality reduction can be achieved with *PCA* or *singular value decomposition*.

Well-known algorithms for *ICA* include

- ***infomax*** ( *learn in detail just this one*)
- *FastICA*
- *JADE*
- *Kernel-independent component analysis*

Main hypothesis: the components of a random vector are generated by a linear combination of independent components. 

$\mathbf{x}​$ = your data

$\mathbf{s}$ = the sources
$$
s=(s_1,s_2,\dots,s_n)^T\\
x = (x_1,x_2,\dots,x_m)^T\\
x_i = a_{i,1}s_1+\dots+a_{i,n}s_n\\
\mathbf{x}=A\mathbf{s}\\
a_k = (a_{1,k},\dots,a_{m,k})\\
A=(a_1,\dots,a_n)^T\\ \ \\\text{Dimensionally speaking...} \ \\\ \\
[\ \mathbf{x}\ ]=m\times \text{nsamples}boooh\\
[\ \mathbf{s}\ ]=n\times m
\\
[\ A\ ]=m\times n
$$
The task obviously is to estimate both the **mixing matrix $A$** and the **sources $s$**

This is done by setting up a cost function which either maximizes the non-gaussianity or minimizes the mutual information of
$$
s_k = (w^Tx)
$$
and compute $w$ recursively. Once the $w$'s are estimated, the original source $s$ can be recovered by multiplying the observed signals $x$ with the inverse of the *mixing matrix* $W=A^{-1}$ also known as the *unmixing matrix* ($n=m$ case, pseudo-inverse is used when $n> m$) 

Usually it'd make sense to add some *Gaussian* noise centered in $0​$ to our formulation or to assume non-linearities:

*Deterministic Model*:
$$
\mathbf{x}=A\mathbf{s}\\
\mathbf{s}=W\mathbf{x}\\
W=A^{-1}
$$
*Noisy model*:
$$
\mathbf{x}=A\mathbf{s}+n\\
n\sim \mathcal{N}(0,\text{diag}(\Sigma))
$$
*Non-Linear model*:
$$
\mathbf{x}=f(\mathbf{s}|\theta)+n\\
$$
To assure the identifiability of an ICA model the following must be assured:

- All the independent components $s_i$, with the possible exception of one component, must be non-Gaussian.
- The number of observed mixtures $m$ must be at least as large as the number of esimated components $n$ ($m\ge n$)
- The matrix $A$ must be of full rank for its inverse to exist.

Let's now define $z = A^Tw$

Then we define $y=w^Tx=w^TAs=z^Ts$

...

**Kurtosis** is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with **high kurtosis** tend to have heavy tails, or outliers. Data sets with **low kurtosis** tend to have light tails, or lack of outliers. A uniform distribution would be the extreme case.